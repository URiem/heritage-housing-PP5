{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aStgWSO0E0E"
   },
   "source": [
    "# Modeling Sale Price using Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eLEkw5O0ECa"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "* Fit and evaluate a regression model to predict the sale price based on the features in the data set.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* outputs/datasets/collection/house_prices_records.csv\n",
    "* Instructions on which variables to use for data cleaning and feature engineering, found in the respective notebooks.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Train set (features and target)\n",
    "* Test set (features and target)\n",
    "* ML pipeline to predict sale price\n",
    "* Labels map\n",
    "* Feature importance plot\n",
    "\n",
    "## Additional Comments\n",
    "\n",
    "* In case you have any additional comments that don't fit in the previous bullets, please state them here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uWZXH9LwoQg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqP-UeN-z3i2"
   },
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOGIGS-uz3i2"
   },
   "source": [
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZfF_j-Bz3i4",
    "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MWW8E7lz3i7"
   },
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwHsQRWjz3i9",
    "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
   },
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_xPk_Ijz3i-"
   },
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vz3S-_kjz3jA",
    "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mavJ8DibrcQ"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"outputs/datasets/collection/house_prices_records.csv\")\n",
    "print(df.shape)\n",
    "for col in df.select_dtypes(include='int64').columns:\n",
    "  df[col] = df[col].astype('float64')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY3l0-AxO93d"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFQo3ycuO-v6"
   },
   "source": [
    "# ML Pipeline: Regression\n",
    "\n",
    "## Create the ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Data Cleaning\n",
    "from feature_engine.imputation import MeanMedianImputer, CategoricalImputer\n",
    "from feature_engine.imputation import ArbitraryNumberImputer\n",
    "from feature_engine.outliers   import Winsorizer\n",
    "\n",
    "# Feature Engineering\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "from feature_engine.selection import SmartCorrelatedSelection, DropFeatures\n",
    "from feature_engine import transformation as vt\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# ML algorithms\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "def PipelineOptimization(model):\n",
    "    pipeline_base = Pipeline([\n",
    "\n",
    "        # Data Cleaning - see Data Cleaning Notebook\n",
    "        (\"DropFeatures\", DropFeatures(features_to_drop=['EnclosedPorch', 'WoodDeckSF'])),\n",
    "\n",
    "        (\"ArbitraryNumberImputer\",ArbitraryNumberImputer(arbitrary_number=0, variables=['2ndFlrSF','MasVnrArea'])),\n",
    "\n",
    "        (\"CategoricalImputer\", CategoricalImputer(imputation_method='missing',fill_value='Unf', variables=['GarageFinish','BsmtFinType1'])),\n",
    "        \n",
    "        (\"MedianImputation\", MeanMedianImputer(imputation_method='median', variables=['BedroomAbvGr' , 'GarageYrBlt', 'LotFrontage'])),\n",
    "\n",
    "        # Feature Engineering - see Feature Engineering Notebook\n",
    "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary', \n",
    "                                                     variables=['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual'])),\n",
    "\n",
    "        (\"LogTransformer\", vt.LogTransformer(variables=['1stFlrSF','GrLivArea'])),\n",
    "\n",
    "        (\"YeoJohnsonTransformer\", vt.YeoJohnsonTransformer(variables=['BsmtUnfSF','GarageArea','TotalBsmtSF'])),\n",
    "\n",
    "        (\"PowerTransformer\", vt.PowerTransformer(variables=['LotArea'])),\n",
    "\n",
    "        (\"Winsorizer\",Winsorizer(capping_method='iqr', tail='both', fold=1.5, variables=['GrLivArea'])),\n",
    "                                      \n",
    "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6, selection_method=\"variance\")),\n",
    "\n",
    "        # Feature Scaling\n",
    "        (\"scaler\", StandardScaler()),\n",
    "\n",
    "        # Feature Selection\n",
    "        (\"feat_selection\", SelectFromModel(model)),\n",
    "\n",
    "        # ML Algorithms\n",
    "        (\"model\", model),\n",
    "    ])\n",
    "\n",
    "    return pipeline_base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Class for hyperparamter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "class HyperparameterOptimizationSearch:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
    "            model = PipelineOptimization(self.models[key])\n",
    "\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring)\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params, all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score',\n",
    "                   'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns], self.grid_searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(['SalePrice'], axis=1),\n",
    "    df['SalePrice'],\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape,\n",
    "      \"\\n* Test set:\",  X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search CV - Sklearn\n",
    "\n",
    "## User default hyperparameters first to find the most suitalble algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_quick_search = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=0),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(random_state=0),\n",
    "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=0),\n",
    "    \"AdaBoostRegressor\": AdaBoostRegressor(random_state=0),\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=0),\n",
    "    \"XGBRegressor\": XGBRegressor(random_state=0),\n",
    "}\n",
    "\n",
    "params_quick_search = {\n",
    "    'LinearRegression': {},\n",
    "    \"DecisionTreeRegressor\": {},\n",
    "    \"RandomForestRegressor\": {},\n",
    "    \"ExtraTreesRegressor\": {},\n",
    "    \"AdaBoostRegressor\": {},\n",
    "    \"GradientBoostingRegressor\": {},\n",
    "    \"XGBRegressor\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a hyperparameter optimization search using the default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = HyperparameterOptimizationSearch(models=models_quick_search, params=params_quick_search)\n",
    "search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the results of the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensive Search for most suitable hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining model parameters for a more extensive search\n",
    "\n",
    "models_search = {\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=0),\n",
    "}\n",
    "\n",
    "# documentation to help on hyperparameter list: \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
    "\n",
    "# We will not conduct an extensive search, since the focus\n",
    "# is on how to combine all knowledge in an applied project.\n",
    "# In a workplace project, you may consider more hyperparameters and spend more time in this step\n",
    "\n",
    "params_search = {\n",
    "    \"GradientBoostingRegressor\": {\n",
    "        'model__n_estimators': [100,300],\n",
    "        'model__learning_rate': [1e-1,1e-2,1e-3], \n",
    "        'model__max_depth': [3,10,None],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You may add as many sections as you want, as long as they support your project workflow.\n",
    "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltNetd085qHf"
   },
   "source": [
    "# Push files to Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you do not need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKlnIozA4eQO",
    "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "  # create here your folder\n",
    "  # os.makedirs(name='')\n",
    "except Exception as e:\n",
    "  print(e)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Practitioner Jupyter Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

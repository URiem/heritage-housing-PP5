{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aStgWSO0E0E"
   },
   "source": [
    "# Data Cleaning Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eLEkw5O0ECa"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "* Data Exploration using ProfileReport\n",
    "* Correlation and PPS Analysis\n",
    "* Detailed evaluation of missing data\n",
    "* Imputing strategies for missing data\n",
    "* Split Train and Test Sets\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* outputs/datasets/collection/house_prices_records.csv \n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Generate cleaned Train and Test sets, saved under outputs/datasets/cleaned \n",
    "\n",
    "## Additional Comments\n",
    "\n",
    "* In case you have any additional comments that don't fit in the previous bullets, please state them here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uWZXH9LwoQg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary packages and libraries for the notebook\n",
    "import os\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ppscore as pps\n",
    "# import pingouin as pg\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqP-UeN-z3i2"
   },
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOGIGS-uz3i2"
   },
   "source": [
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZfF_j-Bz3i4",
    "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MWW8E7lz3i7"
   },
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwHsQRWjz3i9",
    "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
   },
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_xPk_Ijz3i-"
   },
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vz3S-_kjz3jA",
    "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mavJ8DibrcQ"
   },
   "source": [
    "# Load collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_path = \"outputs/datasets/collection/house_prices_records.csv\"\n",
    "df = pd.read_csv(df_raw_path)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY3l0-AxO93d"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFQo3ycuO-v6"
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ProfileReport to get more familiar with the dataset. It will tell use variable types and distribution, missing data levels, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_report = ProfileReport(df=df, minimal=True)\n",
    "pandas_report.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report on the data set reveals that of the 24 columns, 20 contain numerical values and 4 text/categorical values. A closer look at the detailed information on each variable shows that two of the numerical variables, OverallCond and OverallQual, are also categorical data, which are encoded numerically. Overall approximately 10% of data are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation and PPS Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We carry out an initial assessment of the relationship between the different variables/features/data. We use custom functions from the CI Walkthrough project 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        mask[abs(df) < threshold] = True\n",
    "\n",
    "        fig, axes = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                    mask=mask, cmap='viridis', annot_kws={\"size\": font_annot}, ax=axes,\n",
    "                    linewidth=0.5\n",
    "                    )\n",
    "        axes.set_yticklabels(df.columns, rotation=0)\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=bool)\n",
    "        mask[abs(df) < threshold] = True\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                         mask=mask, cmap='rocket_r', annot_kws={\"size\": font_annot},\n",
    "                         linewidth=0.05, linecolor='grey')\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def CalculateCorrAndPPS(df):\n",
    "    df_corr_spearman = df.corr(method=\"spearman\")\n",
    "    df_corr_pearson = df.corr(method=\"pearson\")\n",
    "\n",
    "    pps_matrix_raw = pps.matrix(df)\n",
    "    pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
    "\n",
    "    pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
    "    print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
    "    print(pps_score_stats.round(3))\n",
    "\n",
    "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
    "\n",
    "\n",
    "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold,\n",
    "                      figsize=(20, 12), font_annot=8):\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"* Analyse how the target variable for your ML models are correlated with other variables (features and target)\")\n",
    "    print(\"* Analyse multi-colinearity, that is, how the features are correlated among themselves\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
    "    print(\"It evaluates monotonic relationship \\n\")\n",
    "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
    "    print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
    "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
    "    print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
    "          f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
    "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use CalculateCorrAndPPS function to calculate Correlations and Predictive Power Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayCorrAndPPS(df_corr_pearson = df_corr_pearson,\n",
    "                  df_corr_spearman = df_corr_spearman,\n",
    "                  pps_matrix = pps_matrix,\n",
    "                  CorrThreshold = 0.3, PPS_Threshold = 0.2,\n",
    "                  figsize=(12,10), font_annot = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reports on variables with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
    "vars_with_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if vars_with_missing_data:\n",
    "    profile = ProfileReport(df=df[vars_with_missing_data], minimal=True)\n",
    "    profile.to_notebook_iframe()\n",
    "else:\n",
    "    print(\"There are no variables with missing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A closer look at the columns with missing data shows that in total 9 columns are missing data and a total of roughly 26% of cells are missing data. \n",
    "\n",
    "Another, more concise, way to look at the missing data is with the custom function from the CI Walkthrough Project 2, called EvaluateMissingData. It evalutes the number of missing data values for each variable and the overall percentage of missing data for each variable. I returns the values in a padas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateMissingData(df):\n",
    "    missing_data_absolute = df.isnull().sum()\n",
    "    missing_data_percentage = round(missing_data_absolute/len(df)*100, 2)\n",
    "    df_missing_data = (pd.DataFrame(\n",
    "        data={\"RowsWithMissingData\": missing_data_absolute,\n",
    "              \"PercentageOfDataset\": missing_data_percentage,\n",
    "              \"DataType\": df.dtypes}\n",
    "    )\n",
    "                       .sort_values(by=['PercentageOfDataset'], ascending=False)\n",
    "                       .query(\"PercentageOfDataset > 0\")\n",
    "                      )\n",
    "    return df_missing_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateMissingData(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposal for cleaning/imputing missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data collection is expensive and data is valuable, therefore we want to preserve as much of it as possible and instead of dropping data instances or deleting variables from the data set, we will assess how we can best impute the missing values. A detailed discussion of options follows for each variable with missing data.\n",
    "\n",
    "### Enclosed Porch Square Feet - 90.68% of data missing\n",
    "\n",
    "The variable EnclosedPorch is missing a significant amount of data 90.68%. We have two options, firstly we can drop the variable, due to the large amount of missing data. Secondly we can make the assumption that a missing value means that there is no enclosed porch and thus set the missing value to 0. Due to the large percentage of missig values we will initially drop the feature from the analysis.\n",
    "\n",
    "### Wood Deck Area Square Feet - 89.38% of data missing\n",
    "\n",
    "We can employ the same options and reasoning for handling the missing data of the variable WoodDeckSF as we did for the previous variable. As before we will opt to drop this variable due to the significant amount of missing data.\n",
    "\n",
    "### Lot Frontage in Linear Feet - 17.74% of data missing\n",
    "\n",
    "The mean of the data is 70 and the median 69, the std is 24.3. Due to skewness and kurtosis the data do not seem to have a normal distribution and the median should be imputed.\n",
    "\n",
    "### Garage Finish - 11.1% of data missing\n",
    "\n",
    "A more detailed analysis of property attributes related to garage, shows that some of the missing data have a Garage area of 0, meaning there is no garage. The best approach would probably be a proportional assignment based on the already existing distribution of Garage Finish types, or to use a machine model to predict the garage finish based on the wealth of the other features present. For now and for simplicity we will impute the most common value of 'Unfished'. \n",
    "\n",
    "### Basement Finish Type - 7.81% of data missig\n",
    "\n",
    "Similarly to our discussion for the previous variable we can take several approaches to imputing the missing values, for simplicity we will use the most common value 'Unf' (Unfinished).\n",
    "\n",
    "### Bedrooms Above Ground - 6.78% of data missing\n",
    "\n",
    "The data have a mean of 2.9, a median of 3 and a std of 0.8. We can use the median imputation in this case.\n",
    "\n",
    "### 2nd Floor Square Feet 5.89% of data missing\n",
    "\n",
    "Over 50% of the data are 0, indicating that no second floor is present. The assumption can be that the missing data also mean that no second floor is present and we impute 0 for the missing values.\n",
    "\n",
    "### Garage Year Built - 5.55% of data missing\n",
    "\n",
    "Not surprisingly there is a strong correlation between the year the property was built and the year the garage was built, there could thus be an arguement made for using a machine model to predict the year the garage was built and imputing it that way. For implicity we will impute the median for now.\n",
    "\n",
    "### Masonry Veneer Area in Square Feet - 0.55% of data missing\n",
    "\n",
    "Most values that are present are zero indicating that there is no masonry veneer area present at the property, as a result we can assume that this is also the case for the missing values and impute them with 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed look at Garage and Basement attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we replace missing values in the BasementType and GarageFinish categories by the string 'Missing' and plot the type of basement finish against finished and unfinished basement square footage. We can see that some basements in the missing category have finished and some have unfinished square footage, this may give us a more nuanced idea what values to replace the missing data with. However, since the correlation analysis showes that these variables do not have strong correlation with the sale price of the properties it may make sense to simply impute the values of the most common type of basement finish with is unfinished (UNF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_missing = df\n",
    "df_missing['BsmtFinType1']=df_missing['BsmtFinType1'].fillna('Missing')\n",
    "df_missing['GarageFinish']=df_missing['GarageFinish'].fillna('Missing')\n",
    "# df_missing['GarageYrBlt']=df_missing['GarageYrBlt'].fillna(1800)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15,12))\n",
    "sns.boxplot(data=df_missing, x='BsmtFinType1', y='BsmtFinSF1', ax=axes[0,0])\n",
    "sns.boxplot(data=df_missing, x='BsmtFinType1', y='BsmtUnfSF', ax=axes[1,0])\n",
    "sns.boxplot(data=df_missing, x='BsmtFinType1', y='SalePrice', ax=axes[2,0])\n",
    "\n",
    "sns.boxplot(data=df_missing, x='GarageFinish', y='GarageArea', ax=axes[0,1])\n",
    "sns.boxplot(data=df_missing, x='GarageFinish', y='GarageYrBlt', ax=axes[1,1])\n",
    "sns.boxplot(data=df_missing, x='GarageFinish', y='SalePrice', ax=axes[2,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.normality(df, dv='SalePrice',group='BsmtFinType1', alpha =0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.kruskal(data=df, dv='SalePrice', between='BsmtFinType1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the effects of the proposed data cleaning and imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We carry out the following steps in the data cleaning assessment:\n",
    "\n",
    "1. Drop data that will not be used.\n",
    "2. Select an imputation method to apply to specific variables.\n",
    "3. Create a separate dataframe to apply the method\n",
    "4. Assess the effect of the imputation on the variable distribution\n",
    "\n",
    "To clean the data and assess the effects of the cleaning we use the 'DataCleaningEffect()' function from the Code Insititute's Feature Engine module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataCleaningEffect(df_original,df_cleaned,variables_applied_with_method):\n",
    "\n",
    "  flag_count=1 # Indicate plot number\n",
    "  \n",
    "  # distinguish between numerical and categorical variables\n",
    "  categorical_variables = df_original.select_dtypes(exclude=['number']).columns \n",
    "\n",
    "  # scan over variables, \n",
    "    # first on variables that you applied the method\n",
    "    # if the variable is a numerical plot, a histogram if categorical plot a barplot\n",
    "  for set_of_variables in [variables_applied_with_method]:\n",
    "    print(\"\\n=====================================================================================\")\n",
    "    print(f\"* Distribution Effect Analysis After Data Cleaning Method in the following variables:\")\n",
    "    print(f\"{set_of_variables} \\n\\n\")\n",
    "  \n",
    "\n",
    "    for var in set_of_variables:\n",
    "      if var in categorical_variables:  # it is categorical variable: barplot\n",
    "        \n",
    "        df1 = pd.DataFrame({\"Type\":\"Original\",\"Value\":df_original[var]})\n",
    "        df2 = pd.DataFrame({\"Type\":\"Cleaned\",\"Value\":df_cleaned[var]})\n",
    "        dfAux = pd.concat([df1, df2], axis=0)\n",
    "        fig , axes = plt.subplots(figsize=(15, 5))\n",
    "        sns.countplot(hue='Type', data=dfAux, x=\"Value\",palette=['#432371',\"#FAAE7B\"])\n",
    "        axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.legend() \n",
    "\n",
    "      else: # it is numerical variable: histogram\n",
    "\n",
    "        fig , axes = plt.subplots(figsize=(10, 5))\n",
    "        sns.histplot(data=df_original, x=var, color=\"#432371\", label='Original', kde=True,element=\"step\", ax=axes)\n",
    "        sns.histplot(data=df_cleaned, x=var, color=\"#FAAE7B\", label='Cleaned', kde=True,element=\"step\", ax=axes)\n",
    "        axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
    "        plt.legend() \n",
    "\n",
    "      plt.show()\n",
    "      flag_count+= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will drop the varaibles 'EnclosedPorch' and 'WoodDeckSF' since they have more than 80% of data missing from each column.\n",
    "- The imputation approach is **Drop Variables**\n",
    "- Select variables to apply the imputation approach\n",
    "- Create new data frame and apply the imputation to the selected variables\n",
    "- Inspect the new data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropFeatures\n",
    "\n",
    "variables_method = ['EnclosedPorch', 'WoodDeckSF']\n",
    "variables_method\n",
    "\n",
    "imputer = DropFeatures(features_to_drop=variables_method)\n",
    "df_method = imputer.fit_transform(df)\n",
    "df_method.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbitrary Number Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will impute the number '0' for the following variables '2ndFlrSF' and 'MasVnrArea'.\n",
    "- The imputation approach is **ArbitraryNumberImputer**\n",
    "- Select variables to apply the imputation approach\n",
    "- Create new data frame and apply the imputation to the selected variables\n",
    "- Assess the effect of the data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import ArbitraryNumberImputer\n",
    "\n",
    "variables_method = ['2ndFlrSF', 'MasVnrArea']\n",
    "variables_method\n",
    "\n",
    "imputer = ArbitraryNumberImputer(arbitrary_number=0, variables=variables_method)\n",
    "df_method = imputer.fit_transform(df)\n",
    "\n",
    "DataCleaningEffect(df_original=df,\n",
    "                   df_cleaned=df_method,\n",
    "                   variables_applied_with_method=variables_method)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Imputation\n",
    "- We will impute the category 'Unf' for the following variables 'BsmtFinType1' and 'GarageFinish'.\n",
    "- The imputation approach is **CategoricalImputer**\n",
    "- Select variables to apply the imputation approach\n",
    "- Create new data frame and apply the imputation to the selected variables\n",
    "- Assess the effect of the data imputation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import CategoricalImputer\n",
    "\n",
    "variables_method = ['BsmtFinType1', 'GarageFinish']\n",
    "variables_method\n",
    "\n",
    "imputer = CategoricalImputer(imputation_method='missing', fill_value='Unf', variables=variables_method)\n",
    "df_method = imputer.fit_transform(df)\n",
    "\n",
    "DataCleaningEffect(df_original=df,\n",
    "                   df_cleaned=df_method,\n",
    "                   variables_applied_with_method=variables_method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median Imputation\n",
    "- We will impute the median for the following variables 'BedroomAbvGr', 'GarageYrBlt' and 'LotFrontage'.\n",
    "- The imputation approach is **MeanMedianImputer**\n",
    "- Select variables to apply the imputation approach\n",
    "- Create new data frame and apply the imputation to the selected variables\n",
    "- Assess the effect of the data imputation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import MeanMedianImputer\n",
    "\n",
    "variables_method = ['BedroomAbvGr', 'GarageYrBlt', 'LotFrontage']\n",
    "variables_method\n",
    "\n",
    "imputer = MeanMedianImputer(imputation_method='median', variables=variables_method)\n",
    "df_method = imputer.fit_transform(df)\n",
    "\n",
    "DataCleaningEffect(df_original=df,\n",
    "                   df_cleaned=df_method,\n",
    "                   variables_applied_with_method=variables_method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Split\n",
    "- Next we split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TrainSet, TestSet, _, __ = train_test_split(\n",
    "                                        df,\n",
    "                                        df['SalePrice'],\n",
    "                                        test_size=0.2,\n",
    "                                        random_state=0)\n",
    "\n",
    "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we apply the changes from the various imputation methods to the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Variables ['EnclosedPorch', 'WoodDeckSF']\n",
    "variables_method = ['EnclosedPorch', 'WoodDeckSF']\n",
    "imputer = DropFeatures(features_to_drop=variables_method)\n",
    "imputer.fit(TrainSet)\n",
    "TrainSet, TestSet = imputer.transform(TrainSet), imputer.transform(TestSet)\n",
    "\n",
    "# ArbitraryNumberImputer \n",
    "variables_method = ['2ndFlrSF','MasVnrArea']\n",
    "imputer = ArbitraryNumberImputer(arbitrary_number=0, variables=variables_method)\n",
    "imputer.fit(TrainSet)\n",
    "TrainSet, TestSet = imputer.transform(TrainSet) , imputer.transform(TestSet)\n",
    "\n",
    "# CategoricalImputer\n",
    "variables_method = ['BsmtFinType1', 'GarageFinish']\n",
    "imputer = CategoricalImputer(imputation_method='missing', fill_value='Unf', variables=variables_method)\n",
    "imputer.fit(TrainSet)\n",
    "TrainSet, TestSet = imputer.transform(TrainSet) , imputer.transform(TestSet)\n",
    "\n",
    "# MeanMedianImputer\n",
    "variables_method = ['BedroomAbvGr', 'GarageYrBlt', 'LotFrontage']\n",
    "imputer = MeanMedianImputer(imputation_method='median', variables=variables_method)\n",
    "imputer.fit(TrainSet)\n",
    "TrainSet, TestSet = imputer.transform(TrainSet) , imputer.transform(TestSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check for missing data one more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_data = EvaluateMissingData(TrainSet)\n",
    "print(f\"* There are {df_missing_data.shape[0]} variables with missing data \\n\")\n",
    "df_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Data Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.selection import DropFeatures\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "from feature_engine.imputation import CategoricalImputer\n",
    "from feature_engine.imputation import ArbitraryNumberImputer\n",
    "\n",
    "\n",
    "data_cleaning_pipeline = Pipeline([\n",
    "      ( 'DropFeatures', DropFeatures(features_to_drop=['EnclosedPorch', 'WoodDeckSF']) ),\n",
    "      ( 'MeanMedianImputer', MeanMedianImputer(imputation_method='median', variables=['BedroomAbvGr', 'GarageYrBlt', 'LotFrontage']) ),\n",
    "      ( 'CategoricalImputer', CategoricalImputer(imputation_method='missing', fill_value='Unf', variables=['BsmtFinType1', 'GarageFinish']) ),\n",
    "      ( 'ArbitraryNumberImputer', ArbitraryNumberImputer(arbitrary_number=0, variables=['2ndFlrSF','MasVnrArea']) ),\n",
    "])\n",
    "\n",
    "df = data_cleaning_pipeline.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltNetd085qHf"
   },
   "source": [
    "# Push files to Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save the cleaned Test and Train Sets to your workspace.\n",
    "- Then push all files to the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKlnIozA4eQO",
    "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # create here your folder\n",
    "  os.makedirs(name='outputs/datasets/cleaned')\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "\n",
    "# Save the Train Set\n",
    "TrainSet.to_csv(\"outputs/datasets/cleaned/TrainSetCleaned.csv\", index=False)\n",
    "\n",
    "# Save the Test Set\n",
    "TestSet.to_csv(\"outputs/datasets/cleaned/TestSetCleaned.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Practitioner Jupyter Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
